{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fR1vGDU5_-M1"
   },
   "source": [
    "# PREPARACIÓN DEL ENTORNO (LIBRERIAS)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ghi3rKaz7R7R"
   },
   "source": [
    "**Instalación de librerias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "7QfgbvNO6fiB",
    "outputId": "8c216667-6184-4058-bea5-54fe48981c6f"
   },
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install deltalake\n",
    "!pip install pyarrow\n",
    "!pip install deltalake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Azn4-iww3jvL"
   },
   "source": [
    "**Importación de librerias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dN22lUqj3sDv"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib3\n",
    "import certifi\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from pprint import pprint\n",
    "from datetime import datetime, timedelta\n",
    "from deltalake import write_deltalake, DeltaTable\n",
    "from deltalake.exceptions import TableNotFoundError\n",
    "from configparser import ConfigParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QywLl2LazLJ2"
   },
   "source": [
    "# FUNCIONES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAROVNkf75qw"
   },
   "source": [
    "**Función para realizar una solicitud GET a una API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "e9fztZHm7_Nz"
   },
   "outputs": [],
   "source": [
    "# FUNCIÓN PARA REALIZAR GET A ALGUNA API\n",
    "def get_data(base_url, endpoint, data_field, params=None, headers=None):\n",
    "\n",
    "    \"\"\"\n",
    "    REALIZA UNA SOLICITUD GET A UNA API PARA OBTENER DATOS.\n",
    "\n",
    "    PARÁMETROS:\n",
    "    base_url (str): LA URL DE LA BASE API.\n",
    "    endpoint (str): EL ENDPOINT DE LA API AL QUE SE REALIZARÁ LA SOLICITUD.\n",
    "    params (dict): PARÁMETROS DE CONSULTA PARA ENVIAR CON LA SOLICITUD.\n",
    "    data_field (str): EL NOMBRE DEL CAMPO EN EL JSON QUE CONTIENE LOS DATOS.\n",
    "    headers (dict): ENCABEZADOS PARA ENVIAR CON LA SOLICITUD.\n",
    "\n",
    "    RETORNA:\n",
    "    dict: LOS DATOS OBTENIDOS DE LA API EN FORMATO JSON.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # UNIMOS LA URL CON EL ENDPOINT.\n",
    "        endpoint_url = f\"{base_url}/{endpoint}\"\n",
    "\n",
    "        # EJECUTA LA CONSULTA Y LO ALMACENA EN response.\n",
    "\n",
    "        # SI LA base_url ES DISTINTA DE LA URL DE LA API DEL BCRA\n",
    "        if base_url != \"https://api.bcra.gob.ar\":\n",
    "          # ASIGNACIÓN NORMAL DE response\n",
    "          response = requests.get(endpoint_url, params=params, headers=headers)\n",
    "        else:\n",
    "          # ASIGNACIÓN PERSONALIZADA PARA BCRA EVITAR ERRORES DE SSL\n",
    "\n",
    "          # DESACTIVAR LA ADVERTENCIA DE SSL (NO ES RECOMENDABLE)\n",
    "          urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "          # OPCIÓN 1: DESACTIVAR EL verify (NO ES RECOMENDABLE)\n",
    "          response = requests.get(endpoint_url, params=params, headers=headers, verify=False)\n",
    "\n",
    "          # OPCIÓN 2: REALIZAR LA SOLICITUD Y PASAR EL ARCHIVO DE CERTIFICADOS DE 'certifi' PARA LA VERIFICACIÓN SSL\n",
    "          #response = requests.get(endpoint_url, params=params, headers=headers, verify=certifi.where())\n",
    "\n",
    "        # LEVANTA UNA EXCEPCIÓN SI HAY UN ERROR EN LA RESPUESTA HTTP.\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # VERIFICA SI LOS DATOS ESTAN EN FORMATO JSON.\n",
    "        try:\n",
    "            # ACCEDEMOS AL JSON.\n",
    "            data = response.json()\n",
    "            # ACCEDEMOS AL ARREGLO DENTRO DEL JSON SI data_field FUE ASIGNADO.\n",
    "            if data_field != None:\n",
    "              # FUE ASIGNADO.\n",
    "              data = data[data_field]\n",
    "        except:\n",
    "            # NO ESTA EN FORMATO JSON.\n",
    "            print(\"El formato de respuesta no es el esperado\")\n",
    "            # NO RETORNA NADA.\n",
    "            return None\n",
    "\n",
    "        # RETORNA EL JSON.\n",
    "        return data\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # CAPTURA CUALQUIER ERROR DE SOLICITUD (COMO ERRORES HTTP).\n",
    "        print(f\"La petición ha fallado. Código de error : {e}\")\n",
    "        # NO RETORNA NADA.\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4mQYLe948JU"
   },
   "source": [
    "**Función para construir un DATAFRAME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0kkzc7xu463Z"
   },
   "outputs": [],
   "source": [
    "# FUNCIÓN PARA CONTRUIR UN DATAFRAME\n",
    "def build_table(json_data):\n",
    "    \"\"\"\n",
    "    CONSTRUYE UN DATAFRAME EN PANDAS A PARTIR DE DATOS EN FORMATO JSON.\n",
    "\n",
    "    PARÁMETROS:\n",
    "    json_data (dict): LOS DATOS EN FORMATO JSON OBTENIDOS DE UNA API.\n",
    "\n",
    "    RETORNA:\n",
    "    DATAFRAME: UN DATAFRAME DE PANADS QUE OBTIENE LOS DATOS.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TRANSFORMA EL JSON OBTENIDO EN UN DATAFRAME\n",
    "        df = pd.json_normalize(json_data)\n",
    "        # RETORNA EL DATAFRAME\n",
    "        return df\n",
    "    except:\n",
    "        # ERROR\n",
    "        print(\"Los datos no están en el formato esperado\")\n",
    "        # NO RETORNA NADA\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJ5dxEgaZLxZ"
   },
   "source": [
    "**Función para mostrar de forma más personalizada el DATAFRAME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LSO1n5kqWjbn"
   },
   "outputs": [],
   "source": [
    "def improved_dataframe(json_data):\n",
    "    \"\"\"\n",
    "    CONVIERTE UNA LISTA DE JSON EN UN DATAFRAME ESTRUCTURADO.\n",
    "\n",
    "    PARÁMETROS:\n",
    "    json_data (list): UNA LISTA DE DICCIONARIOS DONDE CADA DICCIONARIO CONTIENE 'fecha' Y 'detalle'.\n",
    "\n",
    "    RETORNA:\n",
    "    pd.DataFrame: UN DATAFRAME DE PANDAS CON LAS COLUMNAS ESTRUCTURADAS COMO 'Fecha', 'CodigoMoneda', 'Descripcion', 'TipoCotizacion', y 'TipoPase'.\n",
    "    \"\"\"\n",
    "\n",
    "    # INICIALIZA UNA LISTA VACÍA PARA ALMACENAR LOS DATOS ESTRUCTURADOS\n",
    "    data = []\n",
    "\n",
    "    # ITERA A TRAVÉS DE LOS ELEMENTOS DEL JSON\n",
    "    for entry in json_data:\n",
    "        # EXTRAE LA FECHA (FECHA)\n",
    "        fecha = entry['fecha']\n",
    "\n",
    "        # ITERA SOBRE 'detalle' Y DESESTRUCTURA EL DICIONARIO ANIDADO\n",
    "        for detalle in entry['detalle']:\n",
    "            # CREA UN DICCIONARIO CON LA FECHA Y LOS VALORES DEL DETALLE\n",
    "            data.append({\n",
    "                'fecha': fecha,\n",
    "                'codigoMoneda': detalle['codigoMoneda'],\n",
    "                'descripcion': detalle['descripcion'],\n",
    "                'tipoCotizacion': detalle['tipoCotizacion'],\n",
    "                'tipoPase': detalle['tipoPase']\n",
    "            })\n",
    "\n",
    "    # CONVIERTE LA LISTA DE DICCIONARIOS EN UN DATAFRAME\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # RETORNA EL DATAFRAME ESTRUCTURADO\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHQk_IZV5Mbn"
   },
   "source": [
    "**Función para guardar todos los datos en formato DELTA LAKE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ezrf0dtK5MtP"
   },
   "outputs": [],
   "source": [
    "# CREA UNA TABLA DELTA LAKE NUEVA\n",
    "# SIEMPRE AGREGA\n",
    "def save_data_as_delta(df, path, mode=\"overwrite\", partition_cols=None):\n",
    "    \"\"\"\n",
    "    Guarda un dataframe en formato Delta Lake en la ruta especificada.\n",
    "    A su vez, es capaz de particionar el dataframe por una o varias columnas.\n",
    "    Por defecto, el modo de guardado es \"overwrite\".\n",
    "\n",
    "    Args:\n",
    "      df (pd.DataFrame): El dataframe a guardar.\n",
    "      path (str): La ruta donde se guardará el dataframe en formato Delta Lake.\n",
    "      mode (str): El modo de guardado. Son los modos que soporta la libreria\n",
    "      deltalake: \"overwrite\", \"append\", \"error\", \"ignore\".\n",
    "      partition_cols (list or str): La/s columna/s por las que se particionará el\n",
    "      dataframe. Si no se especifica, no se particionará.\n",
    "    \"\"\"\n",
    "    write_deltalake(\n",
    "        path, df, mode=mode, partition_by=partition_cols\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YesFaOjm5gnt"
   },
   "source": [
    "**Función para guardar solo datos nuevos en formato DELTA LAKE usando MERGE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0yTEg-5w5g59"
   },
   "outputs": [],
   "source": [
    "# FUNCIÓN PARA GUARDAR SOLO NUEVOS DATOS EN FORMATO DELTA LAKE USANDO MERGE\n",
    "# SI HAY COINCIDENCIAS -> NADA\n",
    "# SI NO HAY COINCIDENCIAS -> REGISTRA\n",
    "def save_new_data_as_delta(new_data, data_path, predicate, partition_cols=None):\n",
    "    \"\"\"\n",
    "    Guarda solo nuevos datos en formato Delta Lake usando la operación MERGE,\n",
    "    comparando los datos ya cargados con los datos que se desean almacenar\n",
    "    asegurando que no se guarden registros duplicados.\n",
    "\n",
    "    Args:\n",
    "      new_data (pd.DataFrame): Los datos que se desean guardar.\n",
    "      data_path (str): La ruta donde se guardará el dataframe en formato Delta Lake.\n",
    "      predicate (str): La condición de predicado para la operación MERGE.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "      dt = DeltaTable(data_path)\n",
    "      new_data_pa = pa.Table.from_pandas(new_data)\n",
    "      # SE INSERTAN EN TARGET, DATOS DE SOURCE QUE NO EXISTEN EN TARGET\n",
    "      dt.merge(\n",
    "          source=new_data_pa,\n",
    "          source_alias=\"source\",\n",
    "          target_alias=\"target\",\n",
    "          predicate=predicate\n",
    "      ) \\\n",
    "      .when_not_matched_insert_all() \\\n",
    "      .execute()# NO HAY COINCIDENCIAS, LO INSERTA\n",
    "\n",
    "    # SI NO EXISTE LA TABLA DELTA LAKE, SE GUARDA COMO NUEVA\n",
    "    except TableNotFoundError:\n",
    "      save_data_as_delta(new_data, data_path, partition_cols=partition_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhkJzYSD5r6E"
   },
   "source": [
    "**Función para actualizar los datos ya registrados y agregar los nuevos en formato DELTA LAKE usando MERGE completo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "L1F3m6_J5sSB"
   },
   "outputs": [],
   "source": [
    "# FUNCIÓN QUE HACE EL MARGE COMPLETO\n",
    "# SI HAY COINCIDENCIAS -> ACTUALIZA\n",
    "# SI NO HAY COINCIDENCIAS -> REGISTRA\n",
    "def upsert_data_as_delta(data, data_path, predicate):\n",
    "    \"\"\"\n",
    "    Guardar datos en formato Delta Lake usando la operacion MERGE.\n",
    "    Cuando no haya registros coincidentes, se insertarán nuevos registros.\n",
    "    Cuando haya registros coincidentes, se actualizarán los campos.\n",
    "\n",
    "    Args:\n",
    "      data (pd.DataFrame): Los datos que se desean guardar.\n",
    "      data_path (str): La ruta donde se guardará el dataframe en formato Delta Lake.\n",
    "      predicate (str): La condición de predicado para la operación MERGE.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dt = DeltaTable(data_path)\n",
    "        data_pa = pa.Table.from_pandas(data)\n",
    "        dt.merge(\n",
    "            source=data_pa,\n",
    "            source_alias=\"source\",\n",
    "            target_alias=\"target\",\n",
    "            predicate=predicate\n",
    "        ) \\\n",
    "        .when_matched_update_all() \\\n",
    "        .when_not_matched_insert_all() \\\n",
    "        .execute()\n",
    "    # SI NO EXISTE LA TABLA DELTA LAKE, SE GUARDA COMO NUEVA\n",
    "    except TableNotFoundError:\n",
    "        save_data_as_delta(data, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOKxmMwwxflg"
   },
   "source": [
    "# PREPARACIÓN DEL ENTORNO (KEY/TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HC_1_pqOxq4X"
   },
   "source": [
    "**IMPORTANTE: Recuerda crear un archivo denominado 'pipeline.conf' que dentro debe tener tu key/token para las consultas GET.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nd3gafmQyT1d"
   },
   "source": [
    "**EJEMPLO:**\n",
    "\n",
    "Tu archivo **'pipeline.conf'** debe verse así:\n",
    "\n",
    "```\n",
    "[bcra_api]\n",
    "access_token = TU_TOKEN_API\n",
    "```\n",
    "\n",
    "Puedes **obtener tu key/token** en: https://estadisticasbcra.com/api/registracion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5S1UHqPzeqE"
   },
   "source": [
    "# EXTRACCIÓN FULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8i5xat9zq4m"
   },
   "source": [
    "1. **Asignación de URL, ENDPOINT, NOMBRE DEL CAMPO DEL JSON. y ENCABEZADOS**\n",
    "2. **Llamado a la función get_data.**\n",
    "3. **Almacenar el JSON obtenido en json_data**\n",
    "4. **Asignar el DATAFRAME en df_currency_masters**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PELbmmBbz_Qd"
   },
   "outputs": [],
   "source": [
    "# ASIGNAMOS LA URL.\n",
    "base_url = \"https://api.bcra.gob.ar\"\n",
    "\n",
    "# ASIGNAMOS EL ENDPOINT.\n",
    "endpoint = \"estadisticascambiarias/v1.0/Maestros/Divisas\"\n",
    "\n",
    "# ASIGNAMOS LA KEY/TOKEN (DEPENDIENDO DE LA API).\n",
    "# IMSTANCIAMOS UN CONFIGPARSER, PARA LEER EL ARCHIVO .conf\n",
    "parser = ConfigParser()\n",
    "\n",
    "# LEEMOS EL ARCHIVO\n",
    "parser.read(\"pipeline.conf\")\n",
    "\n",
    "# ACCEDER A LA SECCIÓN QUE TENGA LAS CREDENCIALES NECESARIAS Y GUARDARLAS EN UNA VARIABLE.\n",
    "api_credentials = parser[\"bcra_api\"]\n",
    "\n",
    "# GUARDAMOS EL TOKEN EN LA VARIABLE access_token\n",
    "access_token = api_credentials[\"access_token\"]\n",
    "\n",
    "# ASIGNAMOS FILTRO POR FECHA (OPCIONAL).\n",
    "#start_date = datetime.utcnow()\n",
    "\n",
    "# ASIGNAMOS EL/LOS ENCABEZADOS.\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\",\n",
    "    \"accept\": \"application/json\"\n",
    "}\n",
    "\n",
    "# ASIGNAMOS EL DATAFIELD (SI NO SE ENVÍA ASIGNAR \"None\").\n",
    "data_field = \"results\"\n",
    "\n",
    "# LLAMAMOS A LA FUNCIÓN Y ALMACENAMOS EL json EN LA VARIABLE json_data\n",
    "json_data = get_data(base_url, endpoint, data_field=data_field, headers=headers)\n",
    "\n",
    "# LLAMAMOS A LA FUNCIÓN Y ASIGNAMOS EL DATAFRAME A LA VARIABLE df_currency_masters\n",
    "df_currency_masters = build_table(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CLPAbY_1J_k"
   },
   "source": [
    "**Mostrar el JSON obtenido**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JvWhekhe1Ofa",
    "outputId": "dd301b16-8ee9-4a57-9944-5e3c6f42bb25"
   },
   "outputs": [],
   "source": [
    "# MUESTRA EL JSON\n",
    "pprint(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntoPg5aS1UAF"
   },
   "source": [
    "**Mostrar el DATAFRAME con sus respectivas cabeceras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "g-hKvtzv1X82",
    "outputId": "8b8c866c-0bde-437b-8285-b619d5888f05"
   },
   "outputs": [],
   "source": [
    "# MUESTRA EL DATAFRAME\n",
    "df_currency_masters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldNy3faA8xu8"
   },
   "source": [
    "# ALMACENAMIENTO DE EXTRACCIÓN FULL (BRONZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrBdM6ZufH4D"
   },
   "source": [
    "1. **Creación del directorio donde se almacenarán los DELTALAKE**\n",
    "2. **Llamamos a la función y declaramos el mode en APPEND**\n",
    "\n",
    "El modo de guardado se realizara en **append**, ya que almacenaremos **datos de divisas**, y es muy poco probable que se quite o agrege una nueva.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YfnL2uVHD_jj",
    "outputId": "20109eed-4978-4337-82e7-7300cacea83c"
   },
   "outputs": [],
   "source": [
    "# CREA EL DIRECTORIO (1 POR CADA ENTIDAD)\n",
    "bronze_dir = \"datalake/bronze/bcra_api\"\n",
    "\n",
    "# CREA UNA SUB CARPETA DENTRO DEL DIRECTORIO\n",
    "currency_masters_raw_dir = f\"{bronze_dir}/maestros_divisas\"\n",
    "\n",
    "# LLAMAMOS A LA FUNCIÓN PARA ALMACENAR EL DATAFRAME EN FORMATO DELTALAKE\n",
    "save_data_as_delta(df_currency_masters, currency_masters_raw_dir, mode=\"append\")\n",
    "\n",
    "# A MODO DE CHEQUEO, LEEMOS LOS DATOS GUARDADOS Y CONTAMOS LA CANTIDAD DE FILAS\n",
    "dt = DeltaTable(currency_masters_raw_dir)\n",
    "print(f\"Cantidad de filas: {dt.to_pandas().shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mST4lVsp6tfP"
   },
   "source": [
    "# TRANSFORMACIÓN DE DATOS FULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ywi4BLIzTszI"
   },
   "source": [
    "**Antes de comenzar a transformar los datos, consultamos cuanto espacio en memoria ocupa el DATAFRAME (memory usage). También nos sirve para obtener un vistazo sobre el DATAFRAME para detectar la existencia de valores nulos, tipo de datos, etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ckcSWq9kT2G9",
    "outputId": "6f879ae5-4cbb-4316-b7e2-33b6ad51bce4"
   },
   "outputs": [],
   "source": [
    "# MEMORIA QUE OCUPA -> \"memory_usage\"\n",
    "df_currency_masters.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjKL-EV0NSKB"
   },
   "source": [
    "**Verificación de valores nulos.**\n",
    "1. **Si hay valores nulos en el campo 'codigo', los eliminara.**\n",
    "2. **Si no hay valores nulos, no hara nada.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4VrawSBZNh5k",
    "outputId": "5bbc08b0-4608-4f3e-a295-1f9725f413a3"
   },
   "outputs": [],
   "source": [
    "# VERIFICA SI HAY VALORES NULOS EN EL CAMPO 'codigo'\n",
    "if df_currency_masters[\"codigo\"].isnull().sum() > 0:\n",
    "    # MUESTRO LA CANTIDAD DE VALORES NULOS\n",
    "    print(f\"Hay {df_currency_masters['codigo'].isnull().sum()} valores nulos en el campo 'codigo'.\")\n",
    "    # ELIMINA LAS FILAS CON VALORES NULOS EN EL CAMPO 'codigo\n",
    "    # HAY VALORES NULOS\n",
    "    # ELIMINA LAS FILAS CON VALORES NULOS EN EL CAMPO 'codigo'\n",
    "    df_currency_masters = df_currency_masters.dropna(subset=[\"codigo\"])\n",
    "else:\n",
    "    # NO HAY VALORES NULOS\n",
    "    print(\"No hay valores nulos en el campo 'codigo'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdI-TiavUaOz"
   },
   "source": [
    "**Imputación con valores por defecto (en el caso de que sean datos nulos)**\n",
    "1. **Si hay valores nulos en el campo 'denominacion', los imputara.**\n",
    "2. **Si no hay valores nulos, no hara nada.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kJIHccReUnfG"
   },
   "outputs": [],
   "source": [
    "# DECLARAMOS EL MAPEO DE IMPUTACION\n",
    "imputation_maping = {\n",
    "    # SI EL CAMPO DENOMINACIÓN ESTA VACIO, ASIGNAMOS 'Not found'\n",
    "    \"denominacion\": \"Not found\"\n",
    "}\n",
    "\n",
    "# UTILIZAMOS fillna PARA AFECTAR SOLO LOS VALORES NULOS\n",
    "df_currency_masters = df_currency_masters.fillna(imputation_maping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoGAk094Xodj"
   },
   "source": [
    "**Conversión de tipos de datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Gk5NeHVhXuBd"
   },
   "outputs": [],
   "source": [
    "# DECLARAMOS EL MAPEO DE CONVERSION\n",
    "conversion_mapping = {\n",
    "    # CAMPO codigo SERA STRING\n",
    "    \"codigo\": \"string\",\n",
    "    # CAMPO denominacion SERA STRING\n",
    "    \"denominacion\": \"string\"\n",
    "}\n",
    "\n",
    "# UTILIZAMOS astype PARA AFECTAR EL TIPO DE DATO DE LAS COLUMNAS\n",
    "df_currency_masters = df_currency_masters.astype(conversion_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUlqpnLSba7j"
   },
   "source": [
    "**Eliminación de duplicados**\n",
    "1. **Primero ordenaremos el DATAFRAME por código (alfabético ascendente)**\n",
    "2. **Comparamos los campos 'codigo' y 'denominacion', si hay coincidencias, se eliminara el duplicado.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "tiQE-G10buhX"
   },
   "outputs": [],
   "source": [
    "# ORDENAR EL DATAFRAME POR LA COLUMNA 'codigo' EN ORDEN ALFABÉTICO ASCENDENTE\n",
    "df_currency_masters = df_currency_masters.sort_values(by='codigo', ascending=True)\n",
    "# ELIMINAMOS LAS FILAS DUPLICADAS, CONSERVAMOS EL PRIMER REGISTRO\n",
    "df_currency_masters = df_currency_masters.drop_duplicates(subset=['codigo', 'denominacion'], keep=\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5uiD2j7YWfa"
   },
   "source": [
    "**Volvemos a consultar el espacio en memoria que ocupa el DATAFRAME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qPSDMTOBYfyl",
    "outputId": "9d71c71c-2211-41b2-a6db-7ad69ce0447c"
   },
   "outputs": [],
   "source": [
    "# MEMORIA QUE OCUPA -> \"memory_usage\"\n",
    "df_currency_masters.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-siqXKpfvtT"
   },
   "source": [
    "# ALMACENAMIENTO DE TRANSFORMACIÓN FULL (SILVER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RLYX1Ujf34L"
   },
   "source": [
    "1. **Creación del directorio donde se almacenarán los DELTALAKE**\n",
    "2. **Llamamos a la función y declaramos el mode en APPEND**\n",
    "\n",
    "El modo de guardado se realizara en **append**, ya que almacenaremos **datos de divisas**, y es muy poco probable que se quite o agrege una nueva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BCLBj9SagahP",
    "outputId": "630a2662-0f67-4e61-eabe-d9dd5aaaa486"
   },
   "outputs": [],
   "source": [
    "# CREA EL DIRECTORIO (1 POR CADA ENTIDAD)\n",
    "silver_dir = \"datalake/silver/bcra_api\"\n",
    "\n",
    "# CREA UNA SUB CARPETA DENTRO DEL DIRECTORIO\n",
    "currency_masters_raw_dir = f\"{silver_dir}/maestros_divisas\"\n",
    "\n",
    "# LLAMAMOS A LA FUNCIÓN PARA ALMACENAR EL DATAFRAME EN FORMATO DELTALAKE\n",
    "save_data_as_delta(df_currency_masters, currency_masters_raw_dir, mode=\"append\")\n",
    "\n",
    "# A MODO DE CHEQUEO, LEEMOS LOS DATOS GUARDADOS Y CONTAMOS LA CANTIDAD DE FILAS\n",
    "dt = DeltaTable(currency_masters_raw_dir)\n",
    "print(f\"Cantidad de filas: {dt.to_pandas().shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQjznci2y8D6"
   },
   "source": [
    "# EXTRACCIÓN INCREMENTAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "De-daeD796ix"
   },
   "source": [
    "\n",
    "1.   **Asignación de URL, ENDPOINT, PARAMETROS, NOMBRE DEL CAMPO DEL JSON. y ENCABEZADOS**\n",
    "2.   **Llamado a la función get_data.**\n",
    "3. **Almacenar el JSON obtenido en json_data**\n",
    "4. **Asignar el DATAFRAME en df_quotations**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "EyWP6l-1-CaZ"
   },
   "outputs": [],
   "source": [
    "# ASIGNAMOS LA URL.\n",
    "base_url = \"https://api.bcra.gob.ar\"\n",
    "\n",
    "# ASIGNAMOS EL ENDPOINT.\n",
    "endpoint = \"estadisticascambiarias/v1.0/Cotizaciones/EUR\"\n",
    "\n",
    "# ASIGNAMOS LA KEY/TOKEN (DEPENDIENDO DE LA API).\n",
    "# IMSTANCIAMOS UN CONFIGPARSER, PARA LEER EL ARCHIVO .conf\n",
    "parser = ConfigParser()\n",
    "# LEEMOS EL ARCHIVO\n",
    "parser.read(\"pipeline.conf\")\n",
    "# ACCEDER A LA SECCIÓN QUE TENGA LAS CREDENCIALES NECESARIAS Y GUARDARLAS EN UNA VARIABLE.\n",
    "api_credentials = parser[\"bcra_api\"]\n",
    "# GUARDAMOS EL TOKEN EN LA VARIABLE access_token\n",
    "access_token = api_credentials[\"access_token\"]\n",
    "\n",
    "# ASIGNAMOS FILTRO POR FECHA (OPCIONAL).\n",
    "#start_date = datetime.utcnow()\n",
    "\n",
    "# ASIGNAMOS EL/LOS ENCABEZADOS.\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\",\n",
    "    \"accept\": \"application/json\"\n",
    "}\n",
    "\n",
    "# ASIGNAMOS EL/LOS PARAMETROS.\n",
    "params = {\n",
    "    \"fechadesde\": \"2023-01-01\",\n",
    "    \"fechahasta\": \"2023-01-31\"\n",
    "}\n",
    "\n",
    "# ASIGNAMOS EL DATAFIELD (SI NO SE ENVÍA ASIGNAR \"None\").\n",
    "data_field = \"results\"\n",
    "\n",
    "# LLAMAMOS A LA FUNCIÓN Y ALMACENAMOS EL json EN LA VARIABLE json_data\n",
    "json_data = get_data(base_url, endpoint, data_field=data_field, params=params, headers=headers)\n",
    "\n",
    "# LLAMAMOS A LA FUNCIÓN Y ASIGNAMOS EL DATAFRAME A LA VARIABLE df_quotations\n",
    "df_quotations = build_table(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8oPje90_i1l"
   },
   "source": [
    "**Mostrar el JSON obtenido**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GxIQdx5L_mCq",
    "outputId": "ff49b823-142d-4eab-8865-38dac054c524"
   },
   "outputs": [],
   "source": [
    "# MUESTRA EL JSON\n",
    "pprint(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJyPXL35_rgl"
   },
   "source": [
    "**Mostrar el DATAFRAME con sus respectivas cabeceras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "G_HykPF1_0Wz",
    "outputId": "aad4f63d-b9d4-4e07-9a79-c7b220f8bdcd"
   },
   "outputs": [],
   "source": [
    "# MUESTRA EL DATAFRAME\n",
    "df_quotations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7N8zTe5WYR4V"
   },
   "source": [
    "**Mostrar el DATAFRAME expandido, accediendo al array del campo \"detalle\" y desarmandolo, con cabeceras mas legibles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "PsMD9WGJZk85",
    "outputId": "2124b44b-4a09-4604-a493-c259a0a5167a"
   },
   "outputs": [],
   "source": [
    "# LLAMAMOS A LA FUNCIÓN Y ALMACENAMOS EL DATAFRAME EN LA VARIABLE df_better\n",
    "df_quotations_expanded = improved_dataframe(json_data)\n",
    "\n",
    "# MUESTRA EL DATAFRAME\n",
    "# print(df_better)\n",
    "df_quotations_expanded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twM2Dqe_nS-b"
   },
   "source": [
    "# ALMACENAMIENTO DE EXTRACCIÓN INCREMENTAL (BRONZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2n6XRQpnZV-"
   },
   "source": [
    "1. **Creación del directorio donde se almacenarán los DELTALAKE**\n",
    "2. **Llamamos a la función que usa el operador MERGE para que solo registre datos si no hay coincidencias con los existentes**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "P915TE2moBgT"
   },
   "outputs": [],
   "source": [
    "# COMANDO PARA BORRAR ALGUN DIRECTORIO\n",
    "# !rm -r ruta/del/directorio\n",
    "\n",
    "# METODO PARA PARTICIONAR LA FECHA Y LA HORA\n",
    "\"\"\"\n",
    "df[\"timestamp_measured\"] = pd.to_datetime(df_measurements.timestamp_measured)\n",
    "df[\"fecha\"] = df.timestamp_measured.dt.date\n",
    "df[\"hora\"] = df.timestamp_measured.dt.hour\n",
    "\"\"\"\n",
    "\n",
    "# CREA UNA SUB CARPETA DENTRO DEL DIRECTORIO\n",
    "quotations_raw_dir = f\"{bronze_dir}/Cotizaciones/EUR\"\n",
    "\n",
    "# LLAMAMOS A LA FUNCIÓN PARA ALMACENAR EL DATAFRAME EN FORMATO DELTALAKE\n",
    "# save_data_as_delta(df, quotations_raw_dir, partition_cols=[\"fecha\"])\n",
    "\n",
    "# LLAMAMOS A LA FUNCIÓN QUE APLICA MERGE (INSERTA SOLO CUANDO NO HAY COINCIDENCIAS)\n",
    "save_new_data_as_delta(\n",
    "    # LLAMAMOS AL DATAFRAME\n",
    "    df_quotations_expanded,\n",
    "    # INDICAMOS EL DIRECTORIO\n",
    "    quotations_raw_dir,\n",
    "    # DECLARAMOS LOS PARAMETROS A COMPARAR\n",
    "    \"\"\" target.fecha = source.fecha\n",
    "    AND target.codigoMoneda = source.codigoMoneda\n",
    "    AND target.descripcion = source.descripcion\n",
    "    AND target.tipoCotizacion = source.tipoCotizacion\n",
    "    AND target.tipoPase = source.tipoPase\"\"\",\n",
    "    # DECLARAMOS LA PARTICIÓN QUE TENDRAN LOS REGISTROS\n",
    "    partition_cols=[\"fecha\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhK_bO7og4-M"
   },
   "source": [
    "# TRANSFORMACIÓN DE DATOS INCREMENTAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrfYv23WkBwq"
   },
   "source": [
    "**Antes de comenzar a transformar los datos, consultamos cuanto espacio en memoria ocupa el DATAFRAME (memory usage). También nos sirve para obtener un vistazo sobre el DATAFRAME para detectar la existencia de valores nulos, tipo de datos, etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tu0xzyNgkvhp",
    "outputId": "28eecd8a-bfa2-4e1c-b37f-6cef3e13c6e0"
   },
   "outputs": [],
   "source": [
    "# MEMORIA QUE OCUPA -> \"memory_usage\"\n",
    "df_quotations_expanded.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1iZfraqldL8"
   },
   "source": [
    "**Verificación de valores nulos.**\n",
    "\n",
    "1. **Si hay valores nulos en el campo 'codigoMoneda', los eliminara.**\n",
    "2. **Si no hay valores nulos, no hara nada.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vLhYEbZ9liGW",
    "outputId": "fb885444-cf79-4104-f57a-6399573a19a7"
   },
   "outputs": [],
   "source": [
    "# VERIFICA SI HAY VALORES NULOS EN EL CAMPO 'codigoMoneda'\n",
    "if df_quotations_expanded[\"codigoMoneda\"].isnull().sum() > 0:\n",
    "    # MUESTRO LA CANTIDAD DE VALORES NULOS\n",
    "    print(f\"Hay {df_quotations_expanded['codigoMoneda'].isnull().sum()} valores nulos en el campo 'codigoMoneda'.\")\n",
    "    # HAY VALORES NULOS\n",
    "    # ELIMINA LAS FILAS CON VALORES NULOS EN EL CAMPO 'codigoMoneda'\n",
    "    df_quotations_expanded = df_quotations_expanded.dropna(subset=[\"codigoMoneda\"])\n",
    "else:\n",
    "    # NO HAY VALORES NULOS\n",
    "    print(\"No hay valores nulos en el campo 'codigoMoneda'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1N5m_uEl9X9"
   },
   "source": [
    "**Imputación con valores por defecto (en el caso de que sean datos nulos)**\n",
    "\n",
    "1. **Si hay valores nulos en los campos declarados, los imputara.**\n",
    "2. **Si no hay valores nulos, no hara nada.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "XejjatKImSv5"
   },
   "outputs": [],
   "source": [
    "# DECLARAMOS EL MAPEO DE IMPUTACION\n",
    "imputation_maping_2 = {\n",
    "    # ASIGNAMOS LOS VALORES A IMPUTAR\n",
    "    \"fecha\": \"1900-01-01 00:00:00\",\n",
    "    \"descripcion\": \"Not found\",\n",
    "    \"tipoCotizacion\": -1,\n",
    "    \"tipoPase\": -1\n",
    "}\n",
    "\n",
    "# UTILIZAMOS fillna PARA AFECTAR SOLO LOS VALORES NULOS\n",
    "df_quotations_expanded = df_quotations_expanded.fillna(imputation_maping_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7EeLYu6neyt"
   },
   "source": [
    "**Conversión de tipos de datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "sP03EKUPnm_M"
   },
   "outputs": [],
   "source": [
    "# DECLARAMOS EL MAPEO DE CONVERSION\n",
    "conversion_mapping_2 = {\n",
    "    # CONVERSION DE TIPOS DE DATOS\n",
    "    \"fecha\": \"datetime64[ns]\",\n",
    "    #\"fecha\": \"datetime64[D]\",\n",
    "    \"codigoMoneda\": \"string\",\n",
    "    \"descripcion\": \"string\",\n",
    "    #\"tipoCotizacion\": \"float32\",\n",
    "    \"tipoPase\": \"float32\"\n",
    "}\n",
    "\n",
    "# UTILIZAMOS astype PARA AFECTAR EL TIPO DE DATO DE LAS COLUMNAS\n",
    "df_quotations_expanded = df_quotations_expanded.astype(conversion_mapping_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzDycRDJqfeH"
   },
   "source": [
    "**Eliminación de duplicados**\n",
    "\n",
    "1. **Primero ordenaremos el DATAFRAME por fecha (ascendiente)**\n",
    "2. **Comparamos los campos declarados, y si hay coincidencias, se eliminara el duplicado.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Jqzo3XpJtmep"
   },
   "outputs": [],
   "source": [
    "# ORDENAR EL DATAFRAME POR LA COLUMNA 'fecha EN ORDEN ASCENDENTE\n",
    "df_quotations_expanded = df_quotations_expanded.sort_values(by='fecha', ascending=True)\n",
    "# ELIMINAMOS LAS FILAS DUPLICADAS, CONSERVAMOS EL ULTIMO REGISTRO\n",
    "df_quotations_expanded = df_quotations_expanded.drop_duplicates(subset=['fecha', 'codigoMoneda'], keep=\"last\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3k2m_K4eueDy"
   },
   "source": [
    "**Volvemos a consultar el espacio en memoria que ocupa el DATAFRAME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pi2SGpYSugK_",
    "outputId": "8fbd53d8-4898-41ea-b7bd-10bdf1ff5c32"
   },
   "outputs": [],
   "source": [
    "# MEMORIA QUE OCUPA -> \"memory_usage\"\n",
    "df_quotations_expanded.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMMlzqD6uxX7"
   },
   "source": [
    "# ALMACENAMIENTO DE TRANSFORMACIÓN INCREMENTAL (SILVER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLuASm-fvBDq"
   },
   "source": [
    "1. **Creación del directorio donde se almacenarán los DELTALAKE**\n",
    "2. **Llamamos a la función que usa el operador MERGE para que solo registre datos si no hay coincidencias con los existentes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "MH6cS1sSvWpQ"
   },
   "outputs": [],
   "source": [
    "# METODO PARA PARTICIONAR LA FECHA Y LA HORA\n",
    "\"\"\"\n",
    "df[\"timestamp_measured\"] = pd.to_datetime(df_measurements.timestamp_measured)\n",
    "df[\"fecha\"] = df.timestamp_measured.dt.date\n",
    "df[\"hora\"] = df.timestamp_measured.dt.hour\n",
    "\"\"\"\n",
    "\n",
    "# CREA UNA SUB CARPETA DENTRO DEL DIRECTORIO\n",
    "quotations_raw_dir = f\"{silver_dir}/Cotizaciones/EUR\"\n",
    "\n",
    "# LLAMAMOS A LA FUNCIÓN PARA ALMACENAR EL DATAFRAME EN FORMATO DELTALAKE\n",
    "# save_data_as_delta(df, quotations_raw_dir, partition_cols=[\"fecha\"])\n",
    "\n",
    "# LLAMAMOS A LA FUNCIÓN QUE APLICA MERGE (INSERTA SOLO CUANDO NO HAY COINCIDENCIAS)\n",
    "save_new_data_as_delta(\n",
    "    # LLAMAMOS AL DATAFRAME\n",
    "    df_quotations_expanded,\n",
    "    # INDICAMOS EL DIRECTORIO\n",
    "    quotations_raw_dir,\n",
    "    # DECLARAMOS LOS PARAMETROS A COMPARAR\n",
    "    \"\"\" target.fecha = source.fecha\n",
    "    AND target.codigoMoneda = source.codigoMoneda\n",
    "    AND target.descripcion = source.descripcion\n",
    "    AND target.tipoCotizacion = source.tipoCotizacion\n",
    "    AND target.tipoPase = source.tipoPase\"\"\",\n",
    "    # DECLARAMOS LA PARTICIÓN QUE TENDRAN LOS REGISTROS\n",
    "    partition_cols=[\"fecha\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mh9fWzAd3d3F"
   },
   "source": [
    "# SUMARIZACIÓN DE DATOS INCREMENTAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8O8WjZmN3pNx"
   },
   "source": [
    "1. **Realizamos una sumarización del promedio de los campos 'tipoCotizacion' y 'tipoPase'.**\n",
    "2. **Contamos la cantidad de registros a través del campo 'codigoMoneda'.**\n",
    "3. **Almacenamos las operaciones en el DATAFRAME df_quotations_sumatory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "VhL8_5Do4CFS"
   },
   "outputs": [],
   "source": [
    "# ASIGNAMOS LA FILTRACION EN EL DATAFRAME df_quotations_sumatory\n",
    "# AGRUPAMOS POR 'codigoMoneda'\n",
    "df_quotations_sumatory = df_quotations_expanded.groupby('codigoMoneda').agg(\n",
    "    # CALCULAMOS LOS PROMEDIOS UTILIZANDO 'mean'\n",
    "    promedio_tipoCotizacion=('tipoCotizacion', 'mean'),\n",
    "    promedio_tipoPase=('tipoPase', 'mean'),\n",
    "    # CONTAMOS LOS REGISTROS UTILIZANDO 'count'\n",
    "    cantidad_registros=('codigoMoneda', 'count')\n",
    ").reset_index() # ASEGURA QUE EL RESULTADO ESTE EN FORMATO ADECUADO(CON SUS COLUMNAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmUZGNvD5jw2"
   },
   "source": [
    "**Mostramos el resultado de la ejecución anterior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "GG7-Dedp5oim",
    "outputId": "f4375844-c03a-4c95-94c9-cc81aa2620d1"
   },
   "outputs": [],
   "source": [
    "df_quotations_sumatory.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bhcvAZz8pf3"
   },
   "source": [
    "# ALMACENAMIENTO DE SUMARIZACIÓN DE DATOS INCREMENTALES (GOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOps4Mzn800e"
   },
   "source": [
    "1. **Creación del directorio donde se almacenarán los DELTALAKE**\n",
    "2. **Llamamos a la función que usa el operador MERGE para que solo registre datos si no hay coincidencias con los existentes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "647BTXJ-9Wp1"
   },
   "outputs": [],
   "source": [
    "# METODO PARA PARTICIONAR LA FECHA Y LA HORA\n",
    "\"\"\"\n",
    "df[\"timestamp_measured\"] = pd.to_datetime(df_measurements.timestamp_measured)\n",
    "df[\"fecha\"] = df.timestamp_measured.dt.date\n",
    "df[\"hora\"] = df.timestamp_measured.dt.hour\n",
    "\"\"\"\n",
    "# CREA EL DIRECTORIO (1 POR CADA ENTIDAD)\n",
    "gold_dir = \"datalake/gold/bcra_api\"\n",
    "\n",
    "# CREA UNA SUB CARPETA DENTRO DEL DIRECTORIO\n",
    "quotationsSumatory_raw_dir = f\"{gold_dir}/promedio_cotizaciones/EUR\"\n",
    "\n",
    "# LLAMAMOS A LA FUNCIÓN PARA ALMACENAR EL DATAFRAME EN FORMATO DELTALAKE\n",
    "# save_data_as_delta(df, quotations_raw_dir, partition_cols=[\"fecha\"])\n",
    "\n",
    "# LLAMAMOS A LA FUNCIÓN QUE APLICA MERGE (INSERTA Y ACTUALIZA)\n",
    "upsert_data_as_delta(\n",
    "    # LLAMAMOS AL DATAFRAME\n",
    "    df_quotations_sumatory,\n",
    "    # INDICAMOS EL DIRECTORIO\n",
    "    quotationsSumatory_raw_dir,\n",
    "    # DECLARAMOS LOS PARAMETROS A COMPARAR\n",
    "    \"\"\" target.codigoMoneda = source.codigoMoneda\n",
    "    AND target.promedio_tipoCotizacion = source.promedio_tipoCotizacion\n",
    "    AND target.promedio_tipoPase = source.promedio_tipoPase\n",
    "    AND target.cantidad_registros = source.cantidad_registros\"\"\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5c8HyYU0ioJ"
   },
   "source": [
    "# *DOCUMENTACIÓN DE LA API*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drdgyNjd0nUf"
   },
   "source": [
    "**Documentación de la API utilizada:** https://www.bcra.gob.ar/Catalogo/Content/files/pdf/estadisticascambiarias-v1.pdf"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "fR1vGDU5_-M1",
    "QywLl2LazLJ2",
    "vOKxmMwwxflg",
    "z5S1UHqPzeqE",
    "ldNy3faA8xu8",
    "mST4lVsp6tfP",
    "v-siqXKpfvtT",
    "CQjznci2y8D6",
    "twM2Dqe_nS-b",
    "fhK_bO7og4-M",
    "YMMlzqD6uxX7",
    "Mh9fWzAd3d3F",
    "1bhcvAZz8pf3"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
